-------------- AIDE DOCKER COMPOSE ---------------
construire le docker compose:
docker-compose -p airflow_spark_streaming up -d

forcer a tout rebuild:
docker-compose -p airflow_spark_streaming up --build -d

redemarrer le docker compose:
docker-compose -p airflow_spark_streaming start

arreter le docker compose:
docker-compose -p airflow_spark_streaming stop

supprimer le docker compose et docker associés:
docker-compose -p airflow_spark_streaming down

recréer le docker compose:
docker-compose -p airflow_spark_streaming down
docker-compose -p airflow_spark_streaming up -d

----------------------------------
TUTORIEL PREMIER FONCTIONNEMENT

telecharger la dernière version stable de airflow(deja fait):
https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml

retirer les dags par defaut(deja fait):
sur le docker-compose (deja fait)
AIRFLOW__CORE__LOAD_EXAMPLES: 'false' (deja fait)

creer fichier .env et ecrire:
AIRFLOW_UID=50000

Si connexion postgre avec dag PostgresHook: (deja fait)
écrire connexion postgre dans fichier .env (deja fait)
puis connecter le fichier .env aux dockers en ecrivant dans le docker-compose: (deja fait)
x-airflow-common:
      env_file:
        - .env

Si connexion avec Spark: (deja fait)
ecrire un Dockerfile avec les installation du provider apache-airflow-providers-apache-spark
sur le docker compose:
commenter ceci: image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.0.3}
decommenter ceci: build: .
puis créer l'image Dockerfile avec java et provider

créer les containers
docker-compose -p airflow_spark_streaming up -d
et aller sur
http://localhost:8080/

se connecter avec
airflow
airflow

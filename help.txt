DROP TABLE IF EXISTS btc_eth_gap_avg_3m_indicator;
DROP TABLE IF EXISTS btc_eth_gap_avg_5m_indicator;
DROP TABLE IF EXISTS btc_usd_avg_indicator;
DROP TABLE IF EXISTS eth_usd_avg_indicator;
DROP TABLE IF EXISTS btc_usd;
DROP TABLE IF EXISTS eth_usd;

CREATE TABLE btc_usd (
  id SERIAL PRIMARY KEY,
  price NUMERIC(15,4) NOT NULL,
  datetime_utc TIMESTAMPTZ(0) NOT NULL UNIQUE
);

CREATE TABLE eth_usd (
  id SERIAL PRIMARY KEY,
  price NUMERIC(15,4) NOT NULL,
  datetime_utc TIMESTAMPTZ(0) NOT NULL UNIQUE
);

CREATE TABLE btc_usd_avg_indicator (
    id SERIAL PRIMARY KEY,
    btc_usd_id INT NOT NULL REFERENCES btc_usd(id) ON DELETE CASCADE,
    price_avg_3m NUMERIC(15,4) NOT NULL,
    price_avg_5m NUMERIC(15,4),
    datetime_utc TIMESTAMPTZ(0) NOT NULL UNIQUE
);

CREATE TABLE eth_usd_avg_indicator (
    id SERIAL PRIMARY KEY,
    eth_usd_id INT NOT NULL REFERENCES eth_usd(id) ON DELETE CASCADE,
    price_avg_3m NUMERIC(15,4) NOT NULL,
    price_avg_5m NUMERIC(15,4),
    datetime_utc TIMESTAMPTZ(0) NOT NULL UNIQUE
);

CREATE TABLE btc_eth_gap_avg_5m_indicator (
    id SERIAL PRIMARY KEY,
    btc_eth_gap_avg_5m NUMERIC(15,4),
    datetime_utc TIMESTAMPTZ(0) NOT NULL UNIQUE
);

CREATE TABLE btc_eth_gap_avg_3m_indicator (
    id SERIAL PRIMARY KEY,
    btc_eth_gap_avg_3m NUMERIC(15,4),
    datetime_utc TIMESTAMPTZ(0) NOT NULL UNIQUE
);
Voici mes tables postgres.

Je t'explique mon système
- J'utilise yfinance en websocket pour récupérer btc_usd et eth_usd toutes les minutes et les loader sur postgres.
- J'utilise airflow toutes les minutes pour récupérer btc_usd sur postgres, calculer la moyenne mobile 3 et 5 minutes, puis écrire le résultat sur postgres sur une nouvelle table.
- J'utilise airflow toutes les minutes pour récupérer btc_usd et eth_usd sur postgres sur les 3 dernières minutes, faire la différences entre btc et eth sur chaque minute de manière distribué avec spark(1master 2 workers) en map, faire un reduce en faisant la moyenne des différences des 3 minutes, puis insérer le résultat de la moyenne des ecart etdu datetime sur postgres sur une nouvelle table

J'ai donc un fichier python pour l'ingestion, un docker compose pour postres+pgadmin, un docker compose pour airflow3, un docker compose pour un master spark + 2 workers.

J'ai réussi a connecter airflow avec postgres et airflow avec spark master.
Tu as tout compris ?


-------------------
Voici l'architecture de mon projet:
Architecture:
->airflow
    ->docker-compose.yaml
    (docker-compose -p airflow_streaming up --build -d)
->ingestion
    ->Dockerfile
    (docker build -t ingestion_streaming . ; docker run -d --name ingestion_streaming ingestion_streaming)
->mailhog
    ->docker-compose.yaml
    (docker-compose -p mailhog_streaming up --build -d)
->postgres
    ->docker-compose.yaml
    (docker-compose -p postgres_streaming up --build -d)
->spark
    ->docker-compose.yaml
    (docker-compose -p spark_streaming up --build -d)

L'objectif est de lancé tout cela avec github action.
L'ordre est important car ils partage tous le meme network de postgres.doit être
L'ordre de création des containers doit être postgres, spark, mailhog, ingestion, airflow

https://docs.github.com/en/actions/how-tos/deploy/configure-and-manage-deployments/control-deployments
